
**Bagging:**  
Bagging trains multiple models on different random subsets of data and aggregates their predictions to reduce variance and prevent overfitting, enhancing model stability. Random Forest is a well-known example.

**Boosting:**  
Boosting sequentially trains models, with each model focusing on correcting the errors of its predecessor, thus reducing bias and variance to build a strong predictive model. Popular algorithms include AdaBoost and XGBoost.

Intro.ipynb: it will shows why ensemble models works better then simple model.
Ensemble Learning(XGBOOST): https://github.com/AmulPatil/Case-Studies/blob/master/natural%20language%20processing/Final_Genre_Classification_winning_solution/Final_Classifying_Movie_Scripts_Predict_The_Movie_Genre.ipynb
[Note:refer this gitrepo which beat transformers in competitions]
