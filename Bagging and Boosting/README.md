
**Bagging:**  
Bagging trains multiple models on different random subsets of data and aggregates their predictions to reduce variance and prevent overfitting, enhancing model stability. Random Forest is a well-known example.

**Boosting:**  
Boosting sequentially trains models, with each model focusing on correcting the errors of its predecessor, thus reducing bias and variance to build a strong predictive model. Popular algorithms include AdaBoost and XGBoost.

Intro.ipynb: it will shows why ensemble models works better then single models algorithms.
